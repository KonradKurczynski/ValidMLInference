{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f59fdb",
   "metadata": {},
   "source": [
    "# CEO Time Use and Firm Performance: A Topic Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1885f",
   "metadata": {},
   "source": [
    "This notebook estimates the association between CEO time allocation and firm performance [(Bandiera et al. 2020)](https://doi.org/10.1086/705331). It illustrates how the functions `ols_bca_topic` and `ols_bcm_topic` can be used to correct bias from estimated topic model shares. The notebook reproduces results from Table 2 of [Battaglia, Christensen, Hansen & Sacher (2024)](https://arxiv.org/abs/2402.15585).\n",
    "\n",
    "[(Bandiera et al. 2020)](https://doi.org/10.1086/705331) conduct a time-use survey for a sample of CEOs. Survey responses are recorded for each 15-minute interval of a given week. The sample consists of 654 answer combinations. To reduce dimensionality, the authros fit a topic model with two topics. One topic places relatively higher mass on features associated with \"management,\" like visiting production sites or meeting with suppliers, while the other places relatively higher mass on features associated with \"leadership\", like communicating with other C-suite executives and holding large, multi-function meetings. \n",
    "\n",
    "Each CEO's leadership weight is a measure of their tendency to engage in leadership activities. One of the key results in [(Bandiera et al. 2020)](https://doi.org/10.1086/705331) is a regression of log sales, a measure of firm size, on the estimateed leadership weight (along with other firm controls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1630154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Import your regression functions\n",
    "from ValidMLInference import (\n",
    "    ols, ols_bca_topic, topic_model_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbe2e7",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The command `topic_model_data()` loads the data we will use in the regression as well as joint estimates of the regression and topic model, as described in [Battaglia, Christensen, Hansen & Sacher (2024)](https://arxiv.org/abs/2402.15585)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11123bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 916\n",
      "Number of control variables: 11\n",
      "Standard deviation of Y: 1.544\n",
      "\n",
      "Sample data:\n",
      "           Y  theta_topic1  control1  control2\n",
      "0  12.352137      0.605186  1.268126       0.0\n",
      "1  10.096356      0.084489  1.113297       0.0\n",
      "2  14.075560      0.969039  3.227946       0.0\n",
      "3  12.358381      0.288178  1.672314       0.0\n",
      "4  10.530302      0.430811  0.728468       0.0\n"
     ]
    }
   ],
   "source": [
    "topic_data = topic_model_data()\n",
    "\n",
    "Z = topic_data['covars']                         # Control variables\n",
    "estimation_data = topic_data['estimation_data']  # Main dataset\n",
    "gamma_draws = topic_data['gamma_draws']          # MCMC draws\n",
    "theta_est_full = topic_data['theta_est_full']    # Full sample topic estimates\n",
    "theta_est_samp = topic_data['theta_est_samp']    # Subsample topic estimates\n",
    "beta_est_full = topic_data['beta_est_full']      # Full sample topic-word distributions\n",
    "beta_est_samp = topic_data['beta_est_samp']      # Subsample topic-word distributions\n",
    "lda_data = topic_data['lda_data']                # Data used to fit the topic model\n",
    "\n",
    "# Dependent variable: log employment, country fixed effects, and survey-wave fixed effects\n",
    "Y = estimation_data['ly']\n",
    "sigma_y = np.std(Y)\n",
    "\n",
    "print(f\"Sample size: {len(Y)}\")\n",
    "print(f\"Number of control variables: {Z.shape[1]}\")\n",
    "print(f\"Standard deviation of Y: {sigma_y:.3f}\")\n",
    "\n",
    "# Show sample of the data\n",
    "sample_data = pd.DataFrame({\n",
    "    'Y': Y,\n",
    "    'theta_topic1': theta_est_full[:, 0],\n",
    "    'control1': Z[:, 0],\n",
    "    'control2': Z[:, 1]\n",
    "})\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "print(sample_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453db0ad",
   "metadata": {},
   "source": [
    "Here `theta_topic1` contains the leadership topic weight for each observation. \n",
    "\n",
    "## Results\n",
    "\n",
    "We first present results for an OLS regression of log sales on the leadership topic weight and controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a082005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Estimates and Confidence Intervals:\n",
      "           Estimate  Std. Error    z value     P>|z|      2.5%      97.5%\n",
      "Intercept  9.874123    0.159194  62.025623  0.000000  9.562108  10.186138\n",
      "topic_1    0.404658    0.092081   4.394608  0.000011  0.224184   0.585133\n"
     ]
    }
   ],
   "source": [
    "# Full sample OLS estimation\n",
    "theta_full = theta_est_full\n",
    "Xhat_full = np.column_stack([theta_full[:, 0], Z])  # First topic + controls\n",
    "\n",
    "# Create variable names\n",
    "var_names = ['topic_1'] + [f'control_{i+1}' for i in range(Z.shape[1])]\n",
    "\n",
    "lm_full = ols(Y=Y, X=Xhat_full, se=True, intercept=True, names=var_names)\n",
    "\n",
    "# Print summary with just the intercept and topic_1 coefficient\n",
    "rows = [\"Intercept\", \"topic_1\"]\n",
    "print(\"OLS Estimates and Confidence Intervals:\")\n",
    "summary = lm_full.summary()\n",
    "print(summary.loc[rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c53ae6",
   "metadata": {},
   "source": [
    "We now compare these estimates with bias-corrected estimates. We will use `ols_bca_topic`. This requires an estimate of κ, which is $\\sqrt{n} \\times E[C_{i}^{-1}]$, where $C_i$ is the number of feature counts in unstructured document $i$. This is stored in the first column of `lda_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b540414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "κ: 0.442\n"
     ]
    }
   ],
   "source": [
    "# Full sample bias correction\n",
    "kappa = np.mean(1.0 / lda_data[:, 0]) * np.sqrt(len(lda_data))\n",
    "print(f\"κ: {kappa:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db5f30",
   "metadata": {},
   "source": [
    "In addition to κ, we need to construct a matrix `S` which picks off the relevant column of `theta_full` (a `n` by `K` matrix, `K` being the number of topcis, here `K = 2`) to include in the regression. \n",
    "\n",
    "We also include the estimated topic-word distributions (a `V` by `K` matrix, `V` being the number of features in the topic model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92943cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias-Corrected Estimates and Confidence Intervals:\n",
      "           Estimate  Std. Error    z value         P>|z|      2.5%      97.5%\n",
      "Intercept  9.842479    0.159194  61.826851  0.000000e+00  9.530464  10.154494\n",
      "topic_1    0.474253    0.092081   5.150410  2.599174e-07  0.293778   0.654728\n"
     ]
    }
   ],
   "source": [
    "# Selection matrix to pick the first topic\n",
    "S = np.array([[1.0, 0.0]])\n",
    "\n",
    "bc_full = ols_bca_topic(\n",
    "    Y=Y,\n",
    "    Q=Z,                    # Control variables\n",
    "    W=theta_est_full,       # Document-topic proportions\n",
    "    S=S,                    # Selection matrix\n",
    "    B=beta_est_full,        # Estimated topic-word distributions\n",
    "    k=kappa,                # Scaling parameter\n",
    "    intercept=True\n",
    ")\n",
    "\n",
    "print(\"Bias-Corrected Estimates and Confidence Intervals:\")\n",
    "summary = bc_full.summary()\n",
    "print(summary.loc[rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453ef73",
   "metadata": {},
   "source": [
    "The two methods (`ols` and `ols_bca_topic`) produce similar estimates and confidence intervals. This suggests that measurement error in the estimated topic_1 shares is small enough that it doesn't materially distort inference.\n",
    "\n",
    "To explore this further, we repeat the above taking a 10% subsample of the data used to estimate the topic model. This ensures the estimated topic weights are noisier signals of the true leadership index. Here we are running the same regression as before, just with a noisier value of the topic_1 weight.\n",
    "\n",
    "The data are named as before, with a `_samp` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc1f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Subsample: Bias-Corrected Estimates and Confidence Intervals:\n",
      "           Estimate  Std. Error    z value     P>|z|      2.5%      97.5%\n",
      "Intercept  9.940524    0.170793  58.202244  0.000000  9.605776  10.275272\n",
      "topic_1    0.226714    0.135119   1.677886  0.093369 -0.038114   0.491541\n"
     ]
    }
   ],
   "source": [
    "# 10% Subsample OLS estimation\n",
    "theta_samp = theta_est_samp\n",
    "Xhat_samp = np.column_stack([theta_samp[:, 0], Z])\n",
    "\n",
    "lm_samp = ols(Y=Y, X=Xhat_samp, se=True, intercept=True, names=var_names)\n",
    "\n",
    "print(\"10% Subsample: Bias-Corrected Estimates and Confidence Intervals:\")\n",
    "summary = lm_samp.summary()\n",
    "print(summary.loc[rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebb4df",
   "metadata": {},
   "source": [
    "Evidently, increasing the measurement error in the estimated topic weights reduces the estimated slope coefficient by around 50%. Moreover, OLS confidence intervals for the slope coefficient now include zero.\n",
    "\n",
    "We now compare with bias correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b42822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa (subsample): 4.262\n",
      "10% Subsample: Bias-Corrected Estimates and Confidence Intervals:\n",
      "           Estimate  Std. Error    z value         P>|z|      2.5%     97.5%\n",
      "Intercept  9.511539    0.170793  55.690510  0.000000e+00  9.176791  9.846286\n",
      "topic_1    1.053774    0.135119   7.798883  6.217249e-15  0.788946  1.318602\n"
     ]
    }
   ],
   "source": [
    "# 10% Subsample bias correction\n",
    "kappa_samp = np.mean(1.0 / lda_data[:, 1]) * np.sqrt(len(lda_data))\n",
    "\n",
    "print(f\"Kappa (subsample): {kappa_samp:.3f}\")\n",
    "\n",
    "bc_samp = ols_bca_topic(\n",
    "    Y=Y,\n",
    "    Q=Z,\n",
    "    W=theta_est_samp,\n",
    "    S=S,\n",
    "    B=beta_est_samp,\n",
    "    k=kappa_samp,\n",
    "    intercept=True\n",
    ")\n",
    "\n",
    "print(\"10% Subsample: Bias-Corrected Estimates and Confidence Intervals:\")\n",
    "summary = bc_samp.summary()\n",
    "print(summary.loc[rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a8c74",
   "metadata": {},
   "source": [
    "Performing the bias correction results in a much larger estimated effect size.\n",
    "\n",
    "Finally we tabluate results from the joint estimation performed by [Battaglia, Christensen, Hansen & Sacher (2024)](https://arxiv.org/abs/2402.15585):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c536cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint Estimates and Confidence Intervals:\n",
      "Full Sample:   0.402 [0.240, 0.602]\n",
      "10% Subsample: 0.439 [0.153, 0.711]\n"
     ]
    }
   ],
   "source": [
    "# Joint estimation using MCMC draws (scaled by dependent variable standard deviation)\n",
    "gamma_scaled = gamma_draws * sigma_y\n",
    "gamma_hat_1 = np.mean(gamma_scaled, axis=0)\n",
    "\n",
    "# Calculate empirical confidence intervals from MCMC draws\n",
    "alpha = 0.05\n",
    "ci_lower_1 = np.percentile(gamma_scaled, 100 * alpha/2, axis=0)\n",
    "ci_upper_1 = np.percentile(gamma_scaled, 100 * (1 - alpha/2), axis=0)\n",
    "\n",
    "print(\"Joint Estimates and Confidence Intervals:\")\n",
    "print(f\"Full Sample:   {gamma_hat_1[0]:.3f} [{ci_lower_1[0]:.3f}, {ci_upper_1[0]:.3f}]\")\n",
    "print(f\"10% Subsample: {gamma_hat_1[1]:.3f} [{ci_lower_1[1]:.3f}, {ci_upper_1[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912aa900",
   "metadata": {},
   "source": [
    "We see that unlike OLS estimation, joint estimation is robust to increasing the noise in the estimated topic weight. Both samples produce a similar estimated effect size confidence intervals that exclude zero.\n",
    "\n",
    "Finally, we tabulate all results together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8a653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Methods:\n",
      "       Sample          Method   Estimate     CI_Lower   CI_Upper\n",
      "         Full        Two-Step  0.4046584   0.22418365  0.5851332\n",
      "10% Subsample        Two-Step 0.22671358 -0.038113967 0.49154115\n",
      "         Full Bias Correction 0.47425315    0.2937784 0.65472794\n",
      "10% Subsample Bias Correction   1.053774   0.78894645  1.3186016\n",
      "         Full           Joint   0.401849     0.239983   0.602348\n",
      "10% Subsample           Joint   0.438758     0.153032   0.711071\n"
     ]
    }
   ],
   "source": [
    "# Helper function to get confidence intervals from regression results\n",
    "def get_ci(result, coef_idx=0, alpha=0.05):\n",
    "    coef = result.coef[coef_idx]\n",
    "    se = np.sqrt(result.vcov[coef_idx, coef_idx])\n",
    "    z_crit = stats.norm.ppf(1 - alpha/2)\n",
    "    return coef - z_crit * se, coef + z_crit * se\n",
    "\n",
    "# Two-step results\n",
    "ci_full_lower, ci_full_upper = get_ci(lm_full, 1)  # topic1 is index 1 (after intercept)\n",
    "ci_samp_lower, ci_samp_upper = get_ci(lm_samp, 1)\n",
    "\n",
    "# Bias correction results\n",
    "ci_bc_full_lower, ci_bc_full_upper = get_ci(bc_full, 1)  # topic_1 is index 1\n",
    "ci_bc_samp_lower, ci_bc_samp_upper = get_ci(bc_samp, 1)\n",
    "\n",
    "results_data = [\n",
    "    {\"Sample\": \"Full\", \"Method\": \"Two-Step\", \n",
    "     \"Estimate\": lm_full.coef[1], \"CI_Lower\": ci_full_lower, \"CI_Upper\": ci_full_upper},\n",
    "    {\"Sample\": \"10% Subsample\", \"Method\": \"Two-Step\", \n",
    "     \"Estimate\": lm_samp.coef[1], \"CI_Lower\": ci_samp_lower, \"CI_Upper\": ci_samp_upper},\n",
    "    {\"Sample\": \"Full\", \"Method\": \"Bias Correction\", \n",
    "     \"Estimate\": bc_full.coef[1], \"CI_Lower\": ci_bc_full_lower, \"CI_Upper\": ci_bc_full_upper},\n",
    "    {\"Sample\": \"10% Subsample\", \"Method\": \"Bias Correction\", \n",
    "     \"Estimate\": bc_samp.coef[1], \"CI_Lower\": ci_bc_samp_lower, \"CI_Upper\": ci_bc_samp_upper},\n",
    "    {\"Sample\": \"Full\", \"Method\": \"Joint\", \n",
    "     \"Estimate\": gamma_hat_1[0], \"CI_Lower\": ci_lower_1[0], \"CI_Upper\": ci_upper_1[0]},\n",
    "     {\"Sample\": \"10% Subsample\", \"Method\": \"Joint\", \n",
    "     \"Estimate\": gamma_hat_1[1], \"CI_Lower\": ci_lower_1[1], \"CI_Upper\": ci_upper_1[1]}\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df.round(3)\n",
    "\n",
    "print(\"Comparison of Methods:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
